{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld65n6NaVjxh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the path to your single JSON review file\n",
        "review_file = 'yelp_academic_dataset_review.json'  # Replace with your actual file name\n",
        "user_file = 'yelp_academic_dataset_user.json'  # Replace with your actual file name\n",
        "business_file = 'yelp_academic_dataset_business.json'  # Replace with your actual file name\n",
        "\n",
        "# Load user and business data (assuming they don't change)\n",
        "users_df = pd.read_json(user_file, lines=True)\n",
        "business_df = pd.read_json(business_file, lines=True)\n",
        "\n",
        "# Initialize an empty DataFrame to accumulate the data\n",
        "final_dataset = pd.DataFrame()\n",
        "\n",
        "# Define the batch size (number of records to process in each batch)\n",
        "batch_size = 1000  # You can adjust this based on your system's memory\n",
        "\n",
        "# Create a generator to read the review file in chunks\n",
        "review_chunks = pd.read_json(review_file, lines=True, chunksize=batch_size)\n",
        "\n",
        "# Define a function to process a single chunk\n",
        "def process_chunk(chunk):\n",
        "    # Merge the data based on common identifiers\n",
        "    merged_df = chunk.merge(users_df[['user_id', 'review_count', 'average_stars']], on='user_id', how='left')\n",
        "    merged_df = merged_df.merge(business_df[['business_id', 'stars', 'review_count']], on='business_id', how='left')\n",
        "\n",
        "    # Select the desired features\n",
        "    return merged_df[['stars_x', 'text', 'review_count_x', 'average_stars', 'stars_y', 'review_count_y']]\n",
        "\n",
        "# Initialize a ThreadPoolExecutor for parallel processing\n",
        "with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
        "    # Process each chunk in parallel\n",
        "    futures = []\n",
        "    processed_chunks = 0\n",
        "    progress_bar = tqdm(total=processed_chunks, desc=\"Processing\")\n",
        "\n",
        "    for chunk in review_chunks:\n",
        "        future = executor.submit(process_chunk, chunk)\n",
        "        future.add_done_callback(lambda p: progress_bar.update(1))\n",
        "        futures.append(future)\n",
        "        processed_chunks += 1\n",
        "\n",
        "    # Wait for all futures to complete\n",
        "    for future in futures:\n",
        "        final_dataset = final_dataset.append(future.result(), ignore_index=True)\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "final_dataset.columns = ['review_stars', 'review_text', 'user_review_count', 'user_average_stars', 'business_stars', 'business_review_count']\n",
        "\n",
        "# Save the final dataset to a CSV file if needed\n",
        "final_dataset.to_csv('final_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your final_dataset.csv (replace 'final_dataset.csv' with your actual file path)\n",
        "final_dataset = pd.read_csv('final_dataset.csv')\n",
        "\n",
        "# Initialize an empty DataFrame to store the balanced dataset\n",
        "balanced_dataset = pd.DataFrame()\n",
        "\n",
        "# Define the number of rows needed for each star rating\n",
        "desired_count_per_rating = 4000\n",
        "\n",
        "# Iterate through star ratings from 1 to 5\n",
        "for star_rating in range(1, 6):\n",
        "    # Filter rows with the current star rating\n",
        "    filtered_rows = final_dataset[final_dataset['review_stars'] == star_rating]\n",
        "\n",
        "    # Randomly sample rows to meet the desired count\n",
        "    sampled_rows = filtered_rows.sample(n=desired_count_per_rating, replace=True)\n",
        "\n",
        "    # Concatenate the sampled rows to the balanced dataset\n",
        "    balanced_dataset = pd.concat([balanced_dataset, sampled_rows], ignore_index=True)\n",
        "\n",
        "# Save the balanced dataset to a new CSV file\n",
        "balanced_dataset.to_csv('balanced_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "L971tu1IWDRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with null values\n",
        "balanced_dataset = balanced_dataset.dropna()\n",
        "\n",
        "# Reset the index after removing rows\n",
        "balanced_dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(balanced_dataset)"
      ],
      "metadata": {
        "id": "mkbIi_hTWOUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}